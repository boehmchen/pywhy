{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trace Visualization Demo\n",
    "\n",
    "This notebook demonstrates how to use the trace visualization functions to compare expected and actual traces in instrumentation tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add pywhy to path if needed\n",
    "sys.path.append('..')\n",
    "\n",
    "from pywhy.trace_visualization import (\n",
    "    format_trace, compare_traces, display_trace_comparison, \n",
    "    show_trace_diff, print_trace_comparison\n",
    ")\n",
    "from pywhy.trace_dsl import trace\n",
    "from pywhy.instrumenter import exec_instrumented\n",
    "from pywhy.tracer import get_tracer\n",
    "\n",
    "print(\"Modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Simple Assignment Trace Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create expected trace using DSL\n",
    "expected_trace = (\n",
    "    trace()\n",
    "    .assign(\"x\", 10)\n",
    "    .assign(\"y\", 20) \n",
    "    .assign(\"z\", 30)\n",
    "    .build()\n",
    ")\n",
    "\n",
    "print(\"Expected trace created:\")\n",
    "print(format_trace(expected_trace, \"Expected Assignment Trace\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run actual instrumentation\n",
    "tracer = get_tracer()  # Use global tracer\n",
    "tracer.clear()  # Clear any previous events\n",
    "\n",
    "code = \"\"\"\n",
    "x = 10\n",
    "y = 20\n",
    "z = x + y\n",
    "\"\"\"\n",
    "\n",
    "# Execute with instrumentation - the tracer will automatically collect events\n",
    "exec_instrumented(code)\n",
    "actual_events = tracer.events\n",
    "\n",
    "print(\"Actual trace:\")\n",
    "print(format_trace(actual_events, \"Actual Assignment Trace\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare traces and show diff\n",
    "comparison = compare_traces(actual_events, expected_trace)\n",
    "print(display_trace_comparison(comparison))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Jupyter-specific display function\n",
    "from IPython.display import HTML, display\n",
    "from pywhy.trace_visualization import create_jupyter_trace_display\n",
    "\n",
    "html_output = create_jupyter_trace_display(actual_events, expected_trace, \"Assignment Test\")\n",
    "display(HTML(html_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Function Call Trace Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create expected trace for function calls\n",
    "expected_function_trace = (\n",
    "    trace()\n",
    "    .function_entry(\"add_numbers\", [5, 3])\n",
    "    .assign(\"result\", 8)\n",
    "    .return_event(8)\n",
    "    .assign(\"output\", 8)\n",
    "    .build()\n",
    ")\n",
    "\n",
    "print(\"Expected function trace:\")\n",
    "print(format_trace(expected_function_trace, \"Expected Function Trace\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function code to instrument\n",
    "function_code = \"\"\"\n",
    "def add_numbers(a, b):\n",
    "    result = a + b\n",
    "    return result\n",
    "\n",
    "output = add_numbers(5, 3)\n",
    "\"\"\"\n",
    "\n",
    "# Use global tracer and clear previous events\n",
    "tracer = get_tracer()\n",
    "tracer.clear()\n",
    "\n",
    "exec_instrumented(function_code)\n",
    "actual_function_events = tracer.events\n",
    "\n",
    "print(\"Actual function trace:\")\n",
    "print(format_trace(actual_function_events, \"Actual Function Trace\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show function trace comparison\n",
    "html_output = create_jupyter_trace_display(\n",
    "    actual_function_events, expected_function_trace, \"Function Call Test\"\n",
    ")\n",
    "display(HTML(html_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Using Test-Attached Functions\n",
    "\n",
    "The instrumentation tests now have trace comparison functions attached to them. Here's how to access them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This would be the pattern for accessing test-attached functions\n",
    "# (Requires running the actual tests first)\n",
    "\n",
    "print(\"\"\"\n",
    "To use test-attached trace comparison functions:\n",
    "\n",
    "1. Run a test:\n",
    "   pytest -v tests/test_instrumentation.py::TestBasicInstrumentation::test_simple_assignment_instrumentation\n",
    "\n",
    "2. The test instance will have these functions attached:\n",
    "   - show_assignment_trace_comparison(): Display in Jupyter\n",
    "   - get_assignment_trace_strings(): Get string representations\n",
    "   - print_assignment_traces(): Print to console\n",
    "\n",
    "3. Similar functions are available for other tests:\n",
    "   - show_function_trace_comparison()\n",
    "   - show_recursion_trace_comparison()\n",
    "   - etc.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions for Quick Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_compare_traces(code: str, expected_dsl_builder, test_name: str):\n",
    "    \"\"\"\n",
    "    Helper function to create and compare traces for any code.\n",
    "    \n",
    "    Args:\n",
    "        code: Python code to instrument and trace\n",
    "        expected_dsl_builder: DSL builder for expected trace\n",
    "        test_name: Name for the test\n",
    "    \"\"\"\n",
    "    # Create expected trace\n",
    "    expected_trace = expected_dsl_builder.build()\n",
    "    \n",
    "    # Use global tracer and clear previous events\n",
    "    tracer = get_tracer()\n",
    "    tracer.clear()\n",
    "    \n",
    "    # Run instrumentation\n",
    "    exec_instrumented(code)\n",
    "    actual_events = tracer.events\n",
    "    \n",
    "    # Show comparison\n",
    "    html_output = create_jupyter_trace_display(actual_events, expected_trace, test_name)\n",
    "    display(HTML(html_output))\n",
    "    \n",
    "    return actual_events, expected_trace\n",
    "\n",
    "print(\"Helper function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of helper function\n",
    "loop_code = \"\"\"\n",
    "total = 0\n",
    "for i in range(3):\n",
    "    total += i\n",
    "\"\"\"\n",
    "\n",
    "expected_loop = (\n",
    "    trace()\n",
    "    .assign(\"total\", 0)\n",
    "    .assign(\"i\", 0)\n",
    "    .aug_assign(\"total\", 0)\n",
    "    .assign(\"i\", 1) \n",
    "    .aug_assign(\"total\", 1)\n",
    "    .assign(\"i\", 2)\n",
    "    .aug_assign(\"total\", 3)\n",
    ")\n",
    "\n",
    "actual, expected = create_and_compare_traces(loop_code, expected_loop, \"Loop Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The trace visualization system provides:\n",
    "\n",
    "1. **String formatting** of traces for readable output\n",
    "2. **Diff generation** to show differences between expected and actual traces\n",
    "3. **Jupyter-friendly HTML output** with side-by-side comparison\n",
    "4. **Test-attached functions** that can be called from notebooks\n",
    "5. **Helper functions** for quick trace comparison\n",
    "\n",
    "This makes it easy to:\n",
    "- Debug instrumentation issues\n",
    "- Verify test correctness\n",
    "- Understand trace execution patterns\n",
    "- Create visual comparisons for documentation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
